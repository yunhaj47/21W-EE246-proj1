{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section \\#1: Centralized Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section \\#1.1: LINEAR REGRESSION\n",
    "\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "from data_load import load\n",
    "import scipy.io as io\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (20, 1)\n",
      "Train target shape:  (20,)\n",
      "Validation data shape:  (20, 1)\n",
      "Validation target shape:  (20,)\n",
      "Test data shape:  (20, 1)\n",
      "Test target shape:  (20,)\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from disk and perform preprocessing to prepare it for the linear regression problem.   \n",
    "    \"\"\"\n",
    "    X_train, y_train = load('regression_train.csv')\n",
    "    X_val, y_val = load('regression_val.csv')\n",
    "    X_test, y_test = load('regression_test.csv')\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test= get_data()  \n",
    "\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Validation data shape: ',X_val.shape)\n",
    "print('Validation target shape: ',y_val.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot the training and test data ##\n",
    "\n",
    "plt.plot(X_train, y_train,'o', color='black')\n",
    "plt.plot(X_test, y_test,'o', color='blue')\n",
    "plt.xlabel('Train data')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Linear Regression\n",
    "\n",
    "In the following cells, you will build a linear regression. You will implement its loss function, then subsequently train it with gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the opimal solution using closed form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Regression import Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete loss_and_grad function in Regression.py file and test your results.\n",
    "regression = Regression(m=1, reg_param=0)\n",
    "loss, grad = regression.loss_and_grad(X_train,y_train)\n",
    "print(loss)\n",
    "print(grad)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete train_LR function in Regression.py file \n",
    "loss_history, w = regression.train_LR(X_train,y_train, eta=1e-3,batch_size=20, num_iters=10000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "print(w)\n",
    "print(loss_history[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete closed_form function in Regression.py file\n",
    "loss_2, w_2 = regression.closed_form(X_train, y_train)\n",
    "print(loss_2)\n",
    "print(w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for lambda from set of values given.\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section \\#1.2: Binary Classification \n",
    "\n",
    "Please follow our instructions in the same order to solve the binary classification problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the dataset from disk and perform preprocessing to prepare it for the linear regression problem.   \n",
    "\"\"\"\n",
    "train = io.loadmat('mnist_train')\n",
    "test = io.loadmat('mnist_test')\n",
    "X_train = train['X_train']\n",
    "y_train = np.transpose(train['y_train'])\n",
    "X_test = test['X_test']\n",
    "y_test = np.transpose(test['y_test'])\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Visualize a point in the dataset\n",
    "index = 4000\n",
    "X = np.array(X_train[index], dtype='uint8')\n",
    "X = X.reshape((28, 28))\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "fig.savefig('Sample.pdf')\n",
    "if y_train[index] == 1:\n",
    "    label = 3\n",
    "else:\n",
    "    label = 2\n",
    "print('label is', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron\n",
    "In the following cells, you will build Perceptron Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_train.shape[0] # Number of data point\n",
    "d = X_train.shape[1] # Number of features\n",
    "loss_hist = []\n",
    "W = np.zeros((d,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Implement the perceptron Algorithm and compute the number of misclassified points at each training step\n",
    "# ================================================================ #\n",
    "\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the percentage of misclassified points in the test data for perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression\n",
    "\n",
    "In the following cells, you will build a logistic regression. You will implement its loss function, then subsequently train it with gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Logistic import Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "N,d = X_train.shape\n",
    "logistic = Logistic(d=d, reg_param=0)\n",
    "loss, grad = logistic.loss_and_grad(X_train,y_train)\n",
    "print('Loss function=',loss)\n",
    "print(np.linalg.norm(grad,ord=2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete train_LR function in Logisitc.py file\n",
    "loss_history, w = logistic.train_LR(X_train,y_train, eta=1e-6,batch_size=50, num_iters=5000)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('LR_loss_hist.pdf')\n",
    "print(np.linalg.norm(w,ord=2)**2)\n",
    "print(loss_history[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete predict function in Logisitc.py file and compute the percentage of misclassified points in the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = [1, 50 , 100, 300]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the Logistic regression for different batch size Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "for t in range (0,10):\n",
    "    for m in range(0,len(Batch)):\n",
    "        \n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM\n",
    "\n",
    "In the following cells, you will build SVM. You will implement its loss function, then subsequently train it with mini-batch gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SVM import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete loss_and_grad function in SVM.py file and test your results.\n",
    "N,d = X_train.shape\n",
    "svm = SVM(d=d, reg_param=0)\n",
    "loss, grad = svm.loss_and_grad(X_train,y_train)\n",
    "print('Loss function=',loss)\n",
    "print(np.linalg.norm(grad,ord=2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete train_svm function in SVM.py file \n",
    "loss_history, w = svm.train_svm(X_train,y_train, eta=1e-6,batch_size=50, num_iters=5000)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('svm_loss_hist.pdf')\n",
    "print(np.linalg.norm(w,ord=2)**2)\n",
    "print(loss_history[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete predict function in SVM.py file and compute the percentage of misclassified points in the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "test_err = np.sum((y_test!=y_pred))*100/X_test.shape[0]\n",
    "print(test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = [1, 50 , 100, 300]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the SVM for different batch size Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "for t in range (0,10):\n",
    "    for m in range(0,len(Batch)):\n",
    "        \n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section \\#1.3: Multi-Class Logistic Regression and Adaboost \n",
    "\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0be1d3552bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train target shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test data shape: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test target shape: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_mnist' is not defined"
     ]
    }
   ],
   "source": [
    "from data_loadM import load_mnist\n",
    "X_train,X_test,y_train,y_test=load_mnist()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Visualize a point in the dataset\n",
    "index = 4000\n",
    "X = np.array(X_train[index], dtype='uint8')\n",
    "X = X.reshape((28, 28))\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "fig.savefig('Sample.pdf')\n",
    "print('label is', y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi-Class Logistic Regression\n",
    "\n",
    "In the following cells, you will build a Multi-Class logistic regression. You will implement its loss function, then subsequently train it with gradient descent. You will implement L1 norm regularization, and choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Logistic import Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "logistic = Logistic(dim=[num_classes,num_features], reg_param=0)\n",
    "loss, grad = logistic.loss_and_grad(X_train[:5000],y_train[:5000])\n",
    "print('Loss function=',loss)\n",
    "print('Frobenius norm of grad=',np.linalg.norm(grad))\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete train_LR function in Logistic.py file \n",
    "loss_history, w = logistic.train_LR(X_train,y_train, eta=1e-7,batch_size=200, num_iters=1500)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "print(np.linalg.norm(w))\n",
    "print(loss_history[1499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#Complete predict function in Logistic.py file and compute the trainin error and the test error\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = [0,1e-6,1e-3,1e-2,1e-1,1]\n",
    "train_err =np.zeros((len(reg),1))\n",
    "test_err =np.zeros((len(reg),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "for m in range(0,len(reg)):\n",
    "    pass\n",
    "    \n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "N = X_train.shape[0]\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "train_err = np.zeros((T,1))\n",
    "test_err = np.zeros((T,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# as a function of number of classifiers T for Adaboost Algorithm. \n",
    "# ================================================================ #\n",
    "\n",
    "#Initialize\n",
    "\n",
    "for t in range(0,T):\n",
    "    #Train decision Tree\n",
    "    \n",
    "    #Compute error\n",
    "    \n",
    "    #Compute \\alpha\n",
    "    \n",
    "    #Update weights\n",
    "    \n",
    "    #Predict using Last t classifiers\n",
    "    \n",
    "    #compute test error and train error\n",
    "    \n",
    "# Plot test error and train error in the same plot vs T\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
